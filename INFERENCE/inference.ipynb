{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Notebook\n",
    "This Jupyter Notebook serves as an inference script for a machine learning model designed for relation extraction from textual dataset TACRED. The code presented here is responsible for loading our pre-trained models, processing input data, performing inference, and evaluating the models' performances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data\n",
    " Example Input Test Data\n",
    "This is an example input test data defined as a list of dictionaries representing data instances. This is the format consistent with the TACRED data. The input sentence for testing should be provided in this format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an example input test data represented as a list of dictionaries.\n",
    "# Each dictionary represents a data instance with various attributes such as 'id', 'docid', 'relation', 'token', 'subj_start', 'subj_end', 'obj_start', 'obj_end', 'subj_type', 'obj_type', 'stanford_pos', 'stanford_ner', 'stanford_head', 'stanford_deprel'.\n",
    "# 'id': Unique identifier for the data instance.\n",
    "# 'docid': Identifier for the document containing the data instance.\n",
    "# 'relation': Type of relation between entities.\n",
    "# 'token': List of tokens representing the text of the instance.\n",
    "# 'subj_start', 'subj_end': Start and end indices of the subject entity in the 'token' list.\n",
    "# 'obj_start', 'obj_end': Start and end indices of the object entity in the 'token' list.\n",
    "# 'subj_type', 'obj_type': Types of subject and object entities.\n",
    "# 'stanford_pos': Part-of-speech tags assigned by the Stanford NLP toolkit.\n",
    "# 'stanford_ner': Named entity recognition tags assigned by the Stanford NLP toolkit.\n",
    "# 'stanford_head': Dependency parsing head indices for each token.\n",
    "# 'stanford_deprel': Dependency relation labels for each token.\n",
    "input_test_data = [{'id': '098f6eb6b0421982e87d',\n",
    "                    'docid': 'APW_ENG_20091113.0131',\n",
    "                    'relation': 'per:age',\n",
    "                    'token': ['Sarah', ',', '33', ',', 'agreed', ',', 'citing', 'the', 'weeks', 'when', 'protesters', 'gathered', 'outside', 'their', 'home', ',', 'once', 'even', 'breaking', 'their', 'home', \"'s\", 'front', 'window', '.'],\n",
    "                    'subj_start': 0,\n",
    "                    'subj_end': 0,\n",
    "                    'obj_start': 2,\n",
    "                    'obj_end': 2,\n",
    "                    'subj_type': 'PERSON',\n",
    "                    'obj_type': 'NUMBER',\n",
    "                    'stanford_pos': ['NNP', ',', 'CD', ',', 'VBD', ',', 'VBG', 'DT', 'NNS', 'WRB', 'NNS', 'VBD', 'IN', 'PRP$', 'NN', ',', 'RB', 'RB', 'VBG', 'PRP$', 'NN', 'POS', 'NN', 'NN', '.'],\n",
    "                    'stanford_ner': ['PERSON', 'O', 'NUMBER', 'O', 'O', 'O', 'O', 'DURATION', 'DURATION', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "                    'stanford_head': [5, 1, 1, 1, 0, 5, 5, 9, 7, 12, 12, 9, 15, 15, 12, 12, 19, 19, 12, 21, 24, 21, 24, 19, 5],\n",
    "                    'stanford_deprel': ['nsubj', 'punct', 'amod', 'punct', 'ROOT', 'punct', 'xcomp', 'det', 'dobj', 'advmod', 'nsubj', 'acl:relcl', 'case', 'nmod:poss', 'nmod', 'punct', 'advmod', 'advmod', 'advcl', 'nmod:poss', 'nmod:poss', 'case', 'compound', 'dobj', 'punct']\n",
    "                    }]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b86c6df",
   "metadata": {},
   "source": [
    "## BERT-based Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6499b178",
   "metadata": {},
   "source": [
    "### 1. Importing Libraries\n",
    "These code cell install & import necessary python packages & required libraries for the code execution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33a83d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prachi/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from module1 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a042389",
   "metadata": {},
   "source": [
    "### 2. Preprocess the Dataset\n",
    "The code segment preprocesses test data by converting tokens into numerical representations and obtaining the subject and object positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a26e0321",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_processed_data = [preprocess_data(item) for item in input_test_data]\n",
    "test_dataset = TACREDDataset(test_processed_data)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da2670f",
   "metadata": {},
   "source": [
    "### 3. Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cdeb38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model1.pkl', 'rb') as file:\n",
    "    loaded_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb1aa1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prachi/Library/Python/3.9/lib/python/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 7.6921\n",
      "Validation Accuracy: 0.0000\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "optimizer = AdamW(loaded_model.parameters(), lr=5e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf2da04",
   "metadata": {},
   "source": [
    "### 4. Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0151988a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Label : ['per:age'], Predicted Label : ['no_relation']\n"
     ]
    }
   ],
   "source": [
    "true_labels,predictions = evaluate_model(loaded_model, test_dataloader, device)\n",
    "original_true_labels = label_encoder.inverse_transform(true_labels)\n",
    "original_predictions = label_encoder.inverse_transform(predictions)\n",
    "print(f'True Label : {original_true_labels}, Predicted Label : {original_predictions}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2279d46b",
   "metadata": {},
   "source": [
    "## GNN-based Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4592a3",
   "metadata": {
    "id": "3ps90HCi0KoE"
   },
   "source": [
    "## RE Approach 2 - GNN based relation extraction using Contrastive Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf45db1",
   "metadata": {
    "id": "HgUo2P1KtTPH"
   },
   "source": [
    "### 1. Installing Required Packages\n",
    "These code cell install & import necessary python packages & required libraries for the code execution. Additionally, it imports custom module '**modules**' for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c131d62d",
   "metadata": {
    "id": "Sr-36GTfkXOa"
   },
   "outputs": [],
   "source": [
    "# This code installs necessary Python packages using the pip package manager.\n",
    "# !pip install numpy torch torch-geometric scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8321152e",
   "metadata": {
    "id": "Jw0CdP6Zf68n"
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries for the code execution.\n",
    "import json  # Importing the JSON module for handling JSON data.\n",
    "import random  # Importing the random module for generating random numbers.\n",
    "import numpy as np  # Importing numpy library and aliasing it as np for numerical computations.\n",
    "import torch  # Importing PyTorch library for deep learning.\n",
    "import torch.nn.functional as F  # Importing torch.nn.functional for various neural network operations.\n",
    "from torch.nn import CrossEntropyLoss  # Importing CrossEntropyLoss for computing the loss.\n",
    "from torch_geometric.data import Data, DataLoader  # Importing Data and DataLoader from torch_geometric for graph data handling.\n",
    "from torch_geometric.nn import GATConv, global_mean_pool  # Importing GATConv and global_mean_pool for graph convolution and pooling operations.\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support  # Importing metrics from scikit-learn for evaluation.\n",
    "\n",
    "import modules2  # Importing custom modules for the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ed844c",
   "metadata": {
    "id": "3IjMLz4cutTr"
   },
   "source": [
    "### 2. Test Data Preprocessing\n",
    "The code segment preprocesses test data by converting tokens, POS tags, and NER tags into numerical representations. Graphs are created from the preprocessed test data to represent structured relationships between tokens and features to be passed into the model. DataLoader for the test graphs is created to facilitate batch processing during testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a5072a4",
   "metadata": {
    "id": "IA-rc1Eef68o",
    "outputId": "3a07e959-b75c-4d35-b5c4-5db1f70c11ca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prachi/Library/Python/3.9/lib/python/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "# It reads test data from a JSON file using a custom module 'modules.read_json_file'. This is test data is loaded just for creating\n",
    "# 'pos_tag_to_index, ner_tag_to_index', which are required during preprocessing\n",
    "# test_data = modules.read_json_file('test.json')\n",
    "\n",
    "# It creates tag indices for part-of-speech (POS) tags and named entity recognition (NER) tags from the test data.\n",
    "# The 'create_tag_indices' function generates dictionaries mapping POS and NER tags to numerical indices.\n",
    "pos_tag_to_index, ner_tag_to_index = modules2.create_tag_indices(modules2.test_data)\n",
    "\n",
    "# It preprocesses the test dataset using a custom module 'modules.preprocess_dataset'.\n",
    "# This function converts tokens, POS tags, and NER tags into numerical representations using the dictionaries generated earlier.\n",
    "preprocessed_test_data = modules2.preprocess_dataset(input_test_data, pos_tag_to_index, ner_tag_to_index)\n",
    "\n",
    "# It creates graphs from the preprocessed test data using a custom module 'modules.create_graphs'.\n",
    "# These graphs represent the structured relationships between tokens and their features.\n",
    "graphs_test = modules2.create_graphs(preprocessed_test_data)\n",
    "\n",
    "# It creates a DataLoader for the test graphs to facilitate batch processing during testing.\n",
    "# The DataLoader is initialized with a batch size of 32 and shuffle set to False.\n",
    "test_loader = DataLoader(graphs_test, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef0a1ee",
   "metadata": {
    "id": "iCZ7AyaawLoa"
   },
   "source": [
    "### 3. Loading Trained Model\n",
    "A pre-trained RelationExtractionGNN model is loaded from a specified file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dafb207",
   "metadata": {
    "id": "Rd42sHSvXZtg"
   },
   "outputs": [],
   "source": [
    "# The variable 'model_path' stores the path where the trained model is saved.\n",
    "model_path = 'model2.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7eb9d0d2",
   "metadata": {
    "id": "vxErCLPbXcPA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code segment loads a trained RelationExtractionGNN model from a specified file path.\n",
    "\n",
    "# It first initializes an instance of the RelationExtractionGNN class using a custom module 'modules'.\n",
    "loaded_model = modules2.RelationExtractionGNN(num_node_features=58, num_classes=42)\n",
    "\n",
    "# It loads the saved model state from the file specified by 'model_path' using the torch.load function.\n",
    "# The loaded model state is stored in the 'checkpoint' dictionary.\n",
    "checkpoint = torch.load(model_path)\n",
    "\n",
    "# It then loads the model state dictionary into the initialized model using the load_state_dict method.\n",
    "# This step initializes the model parameters with the saved weights and biases.\n",
    "loaded_model.load_state_dict(checkpoint['model_state_dict'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac54eef2",
   "metadata": {
    "id": "bC2iCYeXzIsC"
   },
   "source": [
    "### 4. Evaluating Model\n",
    "The loaded model is evaluated on the test input, and the true and predicted labels for the input instance are printed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a0449f8",
   "metadata": {
    "id": "tBwkHB96KCoI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Label : per:age, Predicted Label : no_relation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prachi/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/prachi/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# This code snippet evaluates the loaded model on the test dataset and prints the true and predicted labels for a single instance.\n",
    "true_labels, predicted_labels, _, _, _, _ = modules2.evaluate_model(loaded_model, test_loader)\n",
    "print(f'True Label : {modules2.index_to_relation[true_labels[0]]}, Predicted Label : {modules2.index_to_relation[predicted_labels[0]]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

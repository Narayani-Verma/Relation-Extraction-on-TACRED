# -*- coding: utf-8 -*-
"""modules.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bfDOc8a0XqcvHEMHHxDgKt75h6cTvKzo
"""

# Importing necessary Python packages
import json  # For handling JSON data
import random  # For generating random numbers
import numpy as np  # For numerical operations
import torch  # PyTorch library for deep learning
import torch.nn.functional as F  # Functional interface for common operations
from torch.nn import CrossEntropyLoss  # Loss function for classification tasks
from torch_geometric.data import Data, DataLoader  # Handling graph data and DataLoader for batching
from torch_geometric.nn import GATConv, global_mean_pool  # Graph Attention Network layers
from sklearn.metrics import accuracy_score, precision_recall_fscore_support  # Evaluation metrics

def read_json_file(file_path):
    """
    Function to read JSON data from a file.

    Args:
        file_path (str): Path to the JSON file.

    Returns:
        dict: JSON data read from the file.
    """
    with open(file_path, 'r', encoding='utf-8') as file:
        data = json.load(file)
    return data

# Function to add entity markers to tokens
def mark_entities(tokens, subj_start, subj_end, obj_start, obj_end):
    """
    Add entity markers to tokens to indicate subject and object entities.

    Args:
        tokens (list): List of tokens representing the sentence.
        subj_start (int): Start index of the subject entity.
        subj_end (int): End index of the subject entity.
        obj_start (int): Start index of the object entity.
        obj_end (int): End index of the object entity.

    Returns:
        list: Tokens with entity markers added.
    """
    marked_tokens = []
    for idx, token in enumerate(tokens):
        if idx == subj_start:
            marked_tokens.append("<subj>")
        if idx == obj_start:
            marked_tokens.append("<obj>")
        marked_tokens.append(token)
        if idx == subj_end:
            marked_tokens.append("</subj>")
        if idx == obj_end:
            marked_tokens.append("</obj>")
    return marked_tokens

def create_tag_indices(data):
    """
    Create tag indices for part-of-speech (POS) and named entity recognition (NER) tags.

    Args:
        data (list): List of data items containing POS and NER tags.

    Returns:
        dict: Mapping from POS tags to indices.
        dict: Mapping from NER tags to indices.
    """
    pos_tags = set()
    ner_tags = set()
    for item in data:
        pos_tags.update(item['stanford_pos'])
        ner_tags.update(item['stanford_ner'])
    pos_tag_to_index = {tag: idx for idx, tag in enumerate(pos_tags)}
    ner_tag_to_index = {tag: idx for idx, tag in enumerate(ner_tags)}
    return pos_tag_to_index, ner_tag_to_index

def one_hot_encode_tags(tags, tag_to_index):
    """
    One-hot encode tags based on their indices.

    Args:
        tags (list): List of tags to encode.
        tag_to_index (dict): Mapping from tags to indices.

    Returns:
        numpy.ndarray: One-hot encoded representation of the tags.
    """
    encoded_tags = np.zeros(len(tag_to_index))
    for tag in tags:
        index = tag_to_index[tag]
        encoded_tags[index] = 1
    return encoded_tags

def preprocess_dataset(dataset, pos_tag_to_index, ner_tag_to_index):
    """
    Preprocess dataset by adding entity markers to tokens and one-hot encoding POS and NER tags.

    Args:
        dataset (list): List of data items to preprocess.
        pos_tag_to_index (dict): Mapping from POS tags to indices.
        ner_tag_to_index (dict): Mapping from NER tags to indices.

    Returns:
        list: Preprocessed dataset.
    """
    processed_dataset = []
    for item in dataset:
        processed_item = item.copy()
        processed_item['tokens'] = mark_entities(
            item['token'],
            item['subj_start'], item['subj_end'],
            item['obj_start'], item['obj_end']
        )
        processed_item['stanford_pos_one_hot'] = [one_hot_encode_tags([tag], pos_tag_to_index) for tag in item['stanford_pos']]
        processed_item['stanford_ner_one_hot'] = [one_hot_encode_tags([tag], ner_tag_to_index) for tag in item['stanford_ner']]
        processed_dataset.append(processed_item)
    return processed_dataset

class RelationExtractionGNN(torch.nn.Module):
    """
    Relation Extraction Graph Neural Network model.

    Args:
        num_node_features (int): Number of node features.
        num_classes (int): Number of relation classes.

    Attributes:
        conv1 (GATConv): Graph attention layer 1.
        conv2 (GATConv): Graph attention layer 2.
        fc1 (torch.nn.Linear): Fully connected layer 1.
        fc_relation_pred (torch.nn.Linear): Fully connected layer for relation prediction.
    """
    def __init__(self, num_node_features, num_classes):
        super(RelationExtractionGNN, self).__init__()
        # Define layers
        self.conv1 = GATConv(num_node_features, 128)
        self.conv2 = GATConv(128, 64)
        self.fc1 = torch.nn.Linear(64, 32)
        self.fc_relation_pred = torch.nn.Linear(32, num_classes)

    def forward(self, x, edge_index, batch):
        """
        Forward pass of the model.

        Args:
            x (torch.Tensor): Node features.
            edge_index (torch.Tensor): Graph edge indices.
            batch (torch.Tensor): Batch indices.

        Returns:
            tuple: Tuple containing embeddings and relation predictions.
        """
        # Graph convolution layers with ReLU activation
        x = F.relu(self.conv1(x, edge_index))
        x = F.relu(self.conv2(x, edge_index))
        # Global pooling operation
        x = global_mean_pool(x, batch)
        # Fully connected layer with ReLU activation
        embedding = F.relu(self.fc1(x))
        # Final fully connected layer for relation prediction
        relation_pred = self.fc_relation_pred(embedding)
        return embedding, relation_pred

def evaluate_model(model, loader):
    """
    Evaluate the performance of the model on the given data loader.

    Args:
    - model: The model to be evaluated
    - loader: DataLoader containing the dataset for evaluation

    Returns:
    - true_labels: List of true relation labels
    - predicted_labels: List of predicted relation labels
    - accuracy: Accuracy of the model on the evaluation dataset
    - precision: Precision score of the model on the evaluation dataset
    - recall: Recall score of the model on the evaluation dataset
    - f1: F1 score of the model on the evaluation dataset
    """
    model.eval()  # Set the model to evaluation mode
    true_labels = []  # List to store true relation labels
    predicted_labels = []  # List to store predicted relation labels

    with torch.no_grad():  # Turn off gradient tracking during evaluation
        for data in loader:  # Iterate through the data loader
            # Forward pass through the model to get embeddings and predictions
            embedding, output = model(data.x, data.edge_index, data.batch)

            # Predicted labels are obtained by selecting the class with maximum probability
            preds = output.argmax(dim=1)

            # True relation labels are decoded from one-hot encoding
            true_relations = data.edge_attr.argmax(dim=1)

            # Extend the lists with true and predicted labels
            true_labels.extend(true_relations.tolist())
            predicted_labels.extend(preds.tolist())

    # Calculate evaluation metrics
    accuracy = accuracy_score(true_labels, predicted_labels)
    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predicted_labels, average='weighted')

    return true_labels, predicted_labels, accuracy, precision, recall, f1

def relation_to_one_hot(relation):
    """
    Convert relation to one-hot encoding.

    Args:
        relation (str): Relation label.

    Returns:
        torch.Tensor: One-hot encoded tensor representing the relation.
    """
    # Initialize one-hot tensor with zeros
    one_hot = torch.zeros(len(relation_types), dtype=torch.float)
    # Find the index of the relation and set the corresponding position to 1
    index = relation_to_index[relation]
    one_hot[index] = 1
    return one_hot

def get_shortest_dependency_path(dependency_heads, subj_start, obj_start):
    """
    Find the shortest dependency path between subject and object entities.

    Args:
        dependency_heads (list): List of dependency heads for each token.
        subj_start (int): Start index of the subject entity.
        obj_start (int): Start index of the object entity.

    Returns:
        list: Indices representing the shortest dependency path between the entities.
    """
    # Construct a token graph based on dependency heads
    token_graph = {i: [] for i in range(-1, len(dependency_heads))}
    for i, head in enumerate(dependency_heads):
        adjusted_head = head - 1
        if adjusted_head in token_graph:
            token_graph[adjusted_head].append(i)
            token_graph[i].append(adjusted_head)
    # BFS to find the shortest path
    queue = [(subj_start, [subj_start])]
    visited = set()
    while queue:
        current, path = queue.pop(0)
        if current == obj_start:
            return path
        if current in visited:
            continue
        visited.add(current)
        for neighbor in token_graph[current]:
            if neighbor not in visited and neighbor != -1:
                queue.append((neighbor, path + [neighbor]))
    return []

def create_graphs(dataset):
    """
    Create graph data from the dataset.

    Args:
        dataset (list): List of preprocessed data items.

    Returns:
        list: List of graph data objects.
    """
    graphs = []
    for item in dataset:
        entity_to_id = {}
        node_features = []
        edge_index = []
        edge_attr = []
        # Extract node features for subject and object entities
        for entity_key in ['subj', 'obj']:
            entity_info = (item[f'{entity_key}_type'], tuple(item['tokens'][item[f'{entity_key}_start']:item[f'{entity_key}_end']+1]))
            if entity_info not in entity_to_id:
                pos_one_hot = torch.tensor(item['stanford_pos_one_hot'][item[f'{entity_key}_start']], dtype=torch.float)
                ner_one_hot = torch.tensor(item['stanford_ner_one_hot'][item[f'{entity_key}_start']], dtype=torch.float)
                node_feature = torch.cat((pos_one_hot, ner_one_hot), dim=0)
                node_features.append(node_feature)
                entity_to_id[entity_info] = len(node_features) - 1
        # Find the shortest dependency path between subject and object
        dependency_path = get_shortest_dependency_path(item['stanford_head'], item['subj_start'], item['obj_start'])
        # Add node features for tokens in the dependency path
        for token_idx in dependency_path:
            if token_idx not in entity_to_id:
                pos_one_hot = torch.tensor(item['stanford_pos_one_hot'][token_idx], dtype=torch.float)
                ner_one_hot = torch.tensor(item['stanford_ner_one_hot'][token_idx], dtype=torch.float)
                node_feature = torch.cat((pos_one_hot, ner_one_hot), dim=0)
                node_features.append(node_feature)
                entity_to_id[token_idx] = len(node_features) - 1
        # Get IDs for subject and object entities
        subj_id = entity_to_id[(item['subj_type'], tuple(item['tokens'][item['subj_start']:item['subj_end']+1]))]
        obj_id = entity_to_id[(item['obj_type'], tuple(item['tokens'][item['obj_start']:item['obj_end']+1]))]
        # Add edge between subject and object with relation attribute
        edge_index.append([subj_id, obj_id])
        edge_attr.append(relation_to_one_hot(item['relation']))
        # Convert lists to tensors
        node_features = torch.stack(node_features)
        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()
        edge_attr = torch.stack(edge_attr)
        # Create graph data object
        graph_data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr)
        graphs.append(graph_data)
    return graphs

test_data = read_json_file('../DATA/test.json')

# Create a sorted list of unique relation types from the training data
relation_types = sorted(list(set([item['relation'] for item in test_data])))

# Create a mapping from each relation to a unique index based on the training data
relation_to_index = {relation: idx for idx, relation in enumerate(relation_types)}

index_to_relation = {value: key for key, value in relation_to_index.items()}

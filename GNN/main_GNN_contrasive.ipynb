{"cells":[{"cell_type":"markdown","metadata":{"id":"kIu6dP-nyqvY"},"source":["# GNN-Based Relation Extraction with Contrastive Learning\n","This Jupyter notebook presents a comprehensive approach to extracting relations from TACRED text data using Graph Neural Networks (GNN) augmented by contrastive learning. The methodology is structured into several key sections resulting in a model capable of identifying and classifying relations within sentences."]},{"cell_type":"markdown","metadata":{"id":"CwOZM-gmySvG"},"source":["### 1. Setting Up and Utilizing Key Libraries for Graph Neural Networks\n","\n","This set of code cells prepares the environment for graph neural networks. It includes the installation of necessary Python libraries, imports of various packages for data handling, numerical operations, and evaluation metrics. The setup is essential for performing tasks related to deep learning, graph data processing, and performance evaluation."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cnFwS7Yschlk","outputId":"65003888-575e-4bac-e9d1-7f62dfd58be0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: numpy in /Users/srikanthkuppili/anaconda3/lib/python3.11/site-packages (1.24.3)\n","Requirement already satisfied: torch in /Users/srikanthkuppili/anaconda3/lib/python3.11/site-packages (2.1.1)\n","Requirement already satisfied: torch-geometric in /Users/srikanthkuppili/anaconda3/lib/python3.11/site-packages (2.5.0)\n","Requirement already satisfied: scikit-learn in /Users/srikanthkuppili/anaconda3/lib/python3.11/site-packages (1.3.0)\n","Requirement already satisfied: filelock in /Users/srikanthkuppili/anaconda3/lib/python3.11/site-packages (from torch) (3.9.0)\n","Requirement already satisfied: typing-extensions in /Users/srikanthkuppili/anaconda3/lib/python3.11/site-packages (from torch) (4.7.1)\n","Requirement already satisfied: sympy in /Users/srikanthkuppili/anaconda3/lib/python3.11/site-packages (from torch) (1.11.1)\n","Requirement already satisfied: networkx in /Users/srikanthkuppili/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n","Requirement already satisfied: jinja2 in /Users/srikanthkuppili/anaconda3/lib/python3.11/site-packages (from torch) (3.1.2)\n","Requirement already satisfied: fsspec in /Users/srikanthkuppili/anaconda3/lib/python3.11/site-packages (from torch) (2023.4.0)\n","Requirement already satisfied: tqdm in /Users/srikanthkuppili/anaconda3/lib/python3.11/site-packages (from torch-geometric) (4.65.0)\n","Requirement already satisfied: scipy in /Users/srikanthkuppili/anaconda3/lib/python3.11/site-packages (from torch-geometric) (1.11.1)\n","Requirement already satisfied: aiohttp in /Users/srikanthkuppili/anaconda3/lib/python3.11/site-packages (from torch-geometric) (3.8.5)\n","Requirement already satisfied: requests in /Users/srikanthkuppili/anaconda3/lib/python3.11/site-packages (from torch-geometric) (2.31.0)\n","Requirement already satisfied: pyparsing in /Users/srikanthkuppili/anaconda3/lib/python3.11/site-packages (from torch-geometric) (3.0.9)\n","Requirement already satisfied: psutil>=5.8.0 in /Users/srikanthkuppili/anaconda3/lib/python3.11/site-packages (from torch-geometric) (5.9.0)\n","Requirement already satisfied: joblib>=1.1.1 in /Users/srikanthkuppili/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/srikanthkuppili/anaconda3/lib/python3.11/site-packages (from scikit-learn) (2.2.0)\n","Requirement already satisfied: attrs>=17.3.0 in /Users/srikanthkuppili/anaconda3/lib/python3.11/site-packages (from aiohttp->torch-geometric) (22.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/srikanthkuppili/anaconda3/lib/python3.11/site-packages (from aiohttp->torch-geometric) (2.0.4)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /Users/srikanthkuppili/anaconda3/lib/python3.11/site-packages (from aiohttp->torch-geometric) (6.0.2)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/srikanthkuppili/anaconda3/lib/python3.11/site-packages (from aiohttp->torch-geometric) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /Users/srikanthkuppili/anaconda3/lib/python3.11/site-packages (from aiohttp->torch-geometric) (1.8.1)\n","Requirement already satisfied: frozenlist>=1.1.1 in /Users/srikanthkuppili/anaconda3/lib/python3.11/site-packages (from aiohttp->torch-geometric) (1.3.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /Users/srikanthkuppili/anaconda3/lib/python3.11/site-packages (from aiohttp->torch-geometric) (1.2.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /Users/srikanthkuppili/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n","Requirement already satisfied: idna<4,>=2.5 in /Users/srikanthkuppili/anaconda3/lib/python3.11/site-packages (from requests->torch-geometric) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/srikanthkuppili/anaconda3/lib/python3.11/site-packages (from requests->torch-geometric) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /Users/srikanthkuppili/anaconda3/lib/python3.11/site-packages (from requests->torch-geometric) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /Users/srikanthkuppili/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n"]}],"source":["# This line installs necessary Python packages for the code to run\n","!pip install numpy torch torch-geometric scikit-learn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TWtAAI-L4Ch7"},"outputs":[],"source":["# Importing necessary Python packages\n","import json  # For handling JSON data\n","import random  # For generating random numbers\n","import numpy as np  # For numerical operations\n","import torch  # PyTorch library for deep learning\n","import torch.nn.functional as F  # Functional interface for common operations\n","from torch.nn import CrossEntropyLoss  # Loss function for classification tasks\n","from torch_geometric.data import Data, DataLoader  # Handling graph data and DataLoader for batching\n","from torch_geometric.nn import GATConv, global_mean_pool  # Graph Attention Network layers\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support  # Evaluation metrics"]},{"cell_type":"markdown","metadata":{"id":"fYG976Bkz39G"},"source":["### 2. Data Preprocessing and Stratified Sampling\n","\n","This section focuses on two main tasks: reading data from JSON files and creating stratified subsets of data. Initially, we define a function read_json_file to load JSON data into dictionaries. This function is utilized to read training, development, and test datasets. We introduce the select_stratified_subset function, which selects a stratified subset of the data, ensuring an even distribution based on the 'relation' attribute. This ensures our subsets are representative of the original datasets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AnRHz_NOWpb7"},"outputs":[],"source":["def read_json_file(file_path):\n","    \"\"\"\n","    Function to read JSON data from a file.\n","\n","    Args:\n","        file_path (str): Path to the JSON file.\n","\n","    Returns:\n","        dict: JSON data read from the file.\n","    \"\"\"\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        data = json.load(file)\n","    return data\n","\n","# Reading train, dev, and test data from respective JSON files\n","train_data = read_json_file('train.json')\n","dev_data = read_json_file('dev.json')\n","test_data = read_json_file('test.json')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aJfb_YwikRU1"},"outputs":[],"source":["def select_stratified_subset(data, subset_size):\n","    \"\"\"\n","    Function to select a stratified subset of data based on the distribution of relations.\n","\n","    Args:\n","        data (list): List of dictionaries representing data items, each containing a 'relation' key.\n","        subset_size (int): Size of the desired subset.\n","\n","    Returns:\n","        list: Stratified subset of data.\n","    \"\"\"\n","    # Initialize a dictionary to count the occurrences of each relation\n","    relation_counts = {}\n","\n","    # Count occurrences of each relation in the data\n","    for item in data:\n","        relation = item['relation']\n","        if relation not in relation_counts:\n","            relation_counts[relation] = []\n","        relation_counts[relation].append(item)\n","\n","    # Calculate the total number of items across all relations\n","    total_items = sum(len(items) for items in relation_counts.values())\n","\n","    # Calculate the desired number of items to sample for each relation\n","    subset_counts = {relation: int(len(items) / total_items * subset_size) for relation, items in relation_counts.items()}\n","\n","    # Ensure at least one sample per relation if subset size allows\n","    for relation in subset_counts:\n","        if subset_counts[relation] == 0 and subset_size > 0:\n","            subset_counts[relation] = 1\n","            subset_size -= 1  # Adjust subset_size for added samples\n","\n","    # Sample items from each relation to form the subset\n","    subset = []\n","    for relation, items in relation_counts.items():\n","        if subset_counts[relation] > 0:\n","            sampled_items = random.sample(items, subset_counts[relation])\n","            subset.extend(sampled_items)\n","\n","    return subset\n","\n","# Define the desired subset sizes for train, dev, and test datasets\n","train_subset_size = 10000\n","dev_subset_size = 500\n","test_subset_size = 500\n","\n","# Generate stratified subsets for train and dev datasets\n","stratified_train_data = select_stratified_subset(train_data, train_subset_size)\n","stratified_dev_data = select_stratified_subset(dev_data, dev_subset_size)"]},{"cell_type":"markdown","metadata":{"id":"V6NGxr-j0A8z"},"source":["### 3. Preprocessing for GNN-Based Relation Extraction\n","\n","This section prepares data for Graph Neural Network (GNN) based relation extraction with a focus on contrastive learning. It includes steps to mark entity tokens within sentences, create mappings for part-of-speech (POS) and named entity recognition (NER) tags to indices, one-hot encode these tags, and finally preprocess datasets by integrating these features."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HluGWLHAkOk9"},"outputs":[],"source":["# Function to add entity markers to tokens\n","def mark_entities(tokens, subj_start, subj_end, obj_start, obj_end):\n","    \"\"\"\n","    Add entity markers to tokens to indicate subject and object entities.\n","\n","    Args:\n","        tokens (list): List of tokens representing the sentence.\n","        subj_start (int): Start index of the subject entity.\n","        subj_end (int): End index of the subject entity.\n","        obj_start (int): Start index of the object entity.\n","        obj_end (int): End index of the object entity.\n","\n","    Returns:\n","        list: Tokens with entity markers added.\n","    \"\"\"\n","    marked_tokens = []\n","    for idx, token in enumerate(tokens):\n","        if idx == subj_start:\n","            marked_tokens.append(\"<subj>\")\n","        if idx == obj_start:\n","            marked_tokens.append(\"<obj>\")\n","        marked_tokens.append(token)\n","        if idx == subj_end:\n","            marked_tokens.append(\"</subj>\")\n","        if idx == obj_end:\n","            marked_tokens.append(\"</obj>\")\n","    return marked_tokens\n","\n","def create_tag_indices(data):\n","    \"\"\"\n","    Create tag indices for part-of-speech (POS) and named entity recognition (NER) tags.\n","\n","    Args:\n","        data (list): List of data items containing POS and NER tags.\n","\n","    Returns:\n","        dict: Mapping from POS tags to indices.\n","        dict: Mapping from NER tags to indices.\n","    \"\"\"\n","    pos_tags = set()\n","    ner_tags = set()\n","    for item in data:\n","        pos_tags.update(item['stanford_pos'])\n","        ner_tags.update(item['stanford_ner'])\n","    pos_tag_to_index = {tag: idx for idx, tag in enumerate(pos_tags)}\n","    ner_tag_to_index = {tag: idx for idx, tag in enumerate(ner_tags)}\n","    return pos_tag_to_index, ner_tag_to_index\n","\n","def one_hot_encode_tags(tags, tag_to_index):\n","    \"\"\"\n","    One-hot encode tags based on their indices.\n","\n","    Args:\n","        tags (list): List of tags to encode.\n","        tag_to_index (dict): Mapping from tags to indices.\n","\n","    Returns:\n","        numpy.ndarray: One-hot encoded representation of the tags.\n","    \"\"\"\n","    encoded_tags = np.zeros(len(tag_to_index))\n","    for tag in tags:\n","        index = tag_to_index[tag]\n","        encoded_tags[index] = 1\n","    return encoded_tags\n","\n","def preprocess_dataset(dataset, pos_tag_to_index, ner_tag_to_index):\n","    \"\"\"\n","    Preprocess dataset by adding entity markers to tokens and one-hot encoding POS and NER tags.\n","\n","    Args:\n","        dataset (list): List of data items to preprocess.\n","        pos_tag_to_index (dict): Mapping from POS tags to indices.\n","        ner_tag_to_index (dict): Mapping from NER tags to indices.\n","\n","    Returns:\n","        list: Preprocessed dataset.\n","    \"\"\"\n","    processed_dataset = []\n","    for item in dataset:\n","        processed_item = item.copy()\n","        processed_item['tokens'] = mark_entities(\n","            item['token'],\n","            item['subj_start'], item['subj_end'],\n","            item['obj_start'], item['obj_end']\n","        )\n","        processed_item['stanford_pos_one_hot'] = [one_hot_encode_tags([tag], pos_tag_to_index) for tag in item['stanford_pos']]\n","        processed_item['stanford_ner_one_hot'] = [one_hot_encode_tags([tag], ner_tag_to_index) for tag in item['stanford_ner']]\n","        processed_dataset.append(processed_item)\n","    return processed_dataset\n","\n","# Create tag indices for the stratified train data\n","pos_tag_to_index, ner_tag_to_index = create_tag_indices(stratified_train_data)\n","\n","# Preprocess the stratified train and dev data\n","preprocessed_train_data = preprocess_dataset(stratified_train_data, pos_tag_to_index, ner_tag_to_index)\n","preprocessed_dev_data = preprocess_dataset(stratified_dev_data, pos_tag_to_index, ner_tag_to_index)"]},{"cell_type":"markdown","metadata":{"id":"7tZlckeB1CtO"},"source":["### 4. Encoding Relations and Creating Graph Data\n","\n","In this segment, we organize training data relation types into a structured format and convert relations to one-hot encodings for use in GNN models. Additionally, we introduce a method for computing the shortest dependency path between entities, an essential step for understanding the structural relationships in sentences. This helps in the creation of graph data from our preprocessed dataset. These graphs, which encapsulate entities, their relations, and their shortest dependency paths, are crucial for training our GNN-based relation extraction model with contrastive learning techniques."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uru5rYh18mxO"},"outputs":[],"source":["# Create a sorted list of unique relation types from the training data\n","relation_types = sorted(list(set([item['relation'] for item in preprocessed_train_data])))\n","\n","# Create a mapping from each relation to a unique index based on the training data\n","relation_to_index = {relation: idx for idx, relation in enumerate(relation_types)}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9L2SvTFto4O6"},"outputs":[],"source":["def relation_to_one_hot(relation):\n","    \"\"\"\n","    Convert relation to one-hot encoding.\n","\n","    Args:\n","        relation (str): Relation label.\n","\n","    Returns:\n","        torch.Tensor: One-hot encoded tensor representing the relation.\n","    \"\"\"\n","    # Initialize one-hot tensor with zeros\n","    one_hot = torch.zeros(len(relation_types), dtype=torch.float)\n","    # Find the index of the relation and set the corresponding position to 1\n","    index = relation_to_index[relation]\n","    one_hot[index] = 1\n","    return one_hot\n","\n","def get_shortest_dependency_path(dependency_heads, subj_start, obj_start):\n","    \"\"\"\n","    Find the shortest dependency path between subject and object entities.\n","\n","    Args:\n","        dependency_heads (list): List of dependency heads for each token.\n","        subj_start (int): Start index of the subject entity.\n","        obj_start (int): Start index of the object entity.\n","\n","    Returns:\n","        list: Indices representing the shortest dependency path between the entities.\n","    \"\"\"\n","    # Construct a token graph based on dependency heads\n","    token_graph = {i: [] for i in range(-1, len(dependency_heads))}\n","    for i, head in enumerate(dependency_heads):\n","        adjusted_head = head - 1\n","        if adjusted_head in token_graph:\n","            token_graph[adjusted_head].append(i)\n","            token_graph[i].append(adjusted_head)\n","    # BFS to find the shortest path\n","    queue = [(subj_start, [subj_start])]\n","    visited = set()\n","    while queue:\n","        current, path = queue.pop(0)\n","        if current == obj_start:\n","            return path\n","        if current in visited:\n","            continue\n","        visited.add(current)\n","        for neighbor in token_graph[current]:\n","            if neighbor not in visited and neighbor != -1:\n","                queue.append((neighbor, path + [neighbor]))\n","    return []\n","\n","def create_graphs(dataset):\n","    \"\"\"\n","    Create graph data from the dataset.\n","\n","    Args:\n","        dataset (list): List of preprocessed data items.\n","\n","    Returns:\n","        list: List of graph data objects.\n","    \"\"\"\n","    graphs = []\n","    for item in dataset:\n","        entity_to_id = {}\n","        node_features = []\n","        edge_index = []\n","        edge_attr = []\n","        # Extract node features for subject and object entities\n","        for entity_key in ['subj', 'obj']:\n","            entity_info = (item[f'{entity_key}_type'], tuple(item['tokens'][item[f'{entity_key}_start']:item[f'{entity_key}_end']+1]))\n","            if entity_info not in entity_to_id:\n","                pos_one_hot = torch.tensor(item['stanford_pos_one_hot'][item[f'{entity_key}_start']], dtype=torch.float)\n","                ner_one_hot = torch.tensor(item['stanford_ner_one_hot'][item[f'{entity_key}_start']], dtype=torch.float)\n","                node_feature = torch.cat((pos_one_hot, ner_one_hot), dim=0)\n","                node_features.append(node_feature)\n","                entity_to_id[entity_info] = len(node_features) - 1\n","        # Find the shortest dependency path between subject and object\n","        dependency_path = get_shortest_dependency_path(item['stanford_head'], item['subj_start'], item['obj_start'])\n","        # Add node features for tokens in the dependency path\n","        for token_idx in dependency_path:\n","            if token_idx not in entity_to_id:\n","                pos_one_hot = torch.tensor(item['stanford_pos_one_hot'][token_idx], dtype=torch.float)\n","                ner_one_hot = torch.tensor(item['stanford_ner_one_hot'][token_idx], dtype=torch.float)\n","                node_feature = torch.cat((pos_one_hot, ner_one_hot), dim=0)\n","                node_features.append(node_feature)\n","                entity_to_id[token_idx] = len(node_features) - 1\n","        # Get IDs for subject and object entities\n","        subj_id = entity_to_id[(item['subj_type'], tuple(item['tokens'][item['subj_start']:item['subj_end']+1]))]\n","        obj_id = entity_to_id[(item['obj_type'], tuple(item['tokens'][item['obj_start']:item['obj_end']+1]))]\n","        # Add edge between subject and object with relation attribute\n","        edge_index.append([subj_id, obj_id])\n","        edge_attr.append(relation_to_one_hot(item['relation']))\n","        # Convert lists to tensors\n","        node_features = torch.stack(node_features)\n","        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n","        edge_attr = torch.stack(edge_attr)\n","        # Create graph data object\n","        graph_data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr)\n","        graphs.append(graph_data)\n","    return graphs\n","\n","# Create graph data for the preprocessed train and dev data\n","graphs_train = create_graphs(preprocessed_train_data)\n","graphs_dev = create_graphs(preprocessed_dev_data)"]},{"cell_type":"markdown","metadata":{"id":"RlOBtD2v1Z2W"},"source":["### 5. Constructing Graph Pairs for Contrastive Learning\n","\n","We implement methods for forming balanced graph pairs for training a contrastive learning model. We extract relation labels from graph edge attributes and generate sets of positive (similar relation) and negative (dissimilar relation) graph pairs. This approach prepares the model for effective relation extraction using GNNs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4gzJA0_X0T_o"},"outputs":[],"source":["def extract_relation_label(edge_attr):\n","    \"\"\"\n","    Extract the relation label from the edge attributes.\n","\n","    Args:\n","        edge_attr (torch.Tensor): Edge attributes.\n","\n","    Returns:\n","        int: Index of the maximum value in the tensor, representing the relation label.\n","    \"\"\"\n","    return edge_attr.argmax().item()\n","\n","def form_balanced_graph_pairs(graphs, positive_per_graph=2, negative_per_graph=2):\n","    \"\"\"\n","    Form balanced pairs of graphs for training.\n","\n","    Args:\n","        graphs (list): List of graph data objects.\n","        positive_per_graph (int): Number of positive pairs per graph.\n","        negative_per_graph (int): Number of negative pairs per graph.\n","\n","    Returns:\n","        list: List of balanced pairs of graphs.\n","    \"\"\"\n","    # Organize graphs based on their relation labels\n","    relation_to_graphs = {}\n","    for graph in graphs:\n","        label = extract_relation_label(graph.edge_attr)\n","        if label not in relation_to_graphs:\n","            relation_to_graphs[label] = []\n","        relation_to_graphs[label].append(graph)\n","\n","    # Initialize lists for positive and negative pairs\n","    positive_pairs, negative_pairs = [], []\n","\n","    # Extract relation labels\n","    relations = list(relation_to_graphs.keys())\n","\n","    # Generate positive pairs\n","    for relation, graph_list in relation_to_graphs.items():\n","        for graph in graph_list:\n","            possible_positives = [g for g in graph_list if g != graph]\n","            for _ in range(positive_per_graph):\n","                if possible_positives:\n","                    positive_partner = random.choice(possible_positives)\n","                    positive_pairs.append((graph, positive_partner, 1))\n","\n","    # Generate negative pairs\n","    for relation, graph_list in relation_to_graphs.items():\n","        for graph in graph_list:\n","            other_relations = [r for r in relations if r != relation]\n","            for _ in range(negative_per_graph):\n","                if other_relations:\n","                    other_relation = random.choice(other_relations)\n","                    other_graphs = relation_to_graphs[other_relation]\n","                    if other_graphs:\n","                        negative_partner = random.choice(other_graphs)\n","                        negative_pairs.append((graph, negative_partner, 0))\n","\n","    # Combine positive and negative pairs\n","    pairs = positive_pairs + negative_pairs\n","    random.shuffle(pairs)  # Shuffle pairs to mix positive and negative\n","    return pairs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gjGcKjjD0WLL","outputId":"c6934fbf-2af7-4064-d9ce-c8eba346a99a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Pair label: 1, Graph1 Nodes: 8, Graph2 Nodes: 8\n","Pair label: 0, Graph1 Nodes: 5, Graph2 Nodes: 7\n","Pair label: 0, Graph1 Nodes: 7, Graph2 Nodes: 5\n","Pair label: 1, Graph1 Nodes: 10, Graph2 Nodes: 14\n","Pair label: 0, Graph1 Nodes: 7, Graph2 Nodes: 5\n"]}],"source":["balanced_pairs = form_balanced_graph_pairs(graphs_train, positive_per_graph=2, negative_per_graph=2)\n","\n","# Print information about the first 5 pairs\n","for pair in balanced_pairs[:5]:\n","    graph1, graph2, label = pair\n","    # Display label and number of nodes in each graph\n","    print(f\"Pair label: {label}, Graph1 Nodes: {graph1.x.shape[0]}, Graph2 Nodes: {graph2.x.shape[0]}\")"]},{"cell_type":"markdown","metadata":{"id":"skNW5ZAT1gt8"},"source":["### 6. Graph Neural Network Model for Relation Extraction with Contrastive Learning\n","\n","Our approach of implementing GNN model specifically designed for relation extraction, enhanced by contrastive learning consists of a method to compute the contrastive loss between pairs of graph embeddings which discerns between similar and dissimilar relations. The model architecture includes graph attention layers for feature extraction from nodes, followed by global mean pooling and fully connected layers for embedding generation and relation classification."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C_40JLmMChGm"},"outputs":[],"source":["def contrastive_loss(embedding1, embedding2, labels, margin=1.0):\n","    \"\"\"\n","    Compute contrastive loss between pairs of embeddings.\n","\n","    Args:\n","        embedding1 (torch.Tensor): Embeddings of the first item in the pair.\n","        embedding2 (torch.Tensor): Embeddings of the second item in the pair.\n","        labels (torch.Tensor): Labels indicating whether each pair is positive (1) or negative (0).\n","        margin (float): Margin for negative pairs.\n","\n","    Returns:\n","        torch.Tensor: Contrastive loss.\n","    \"\"\"\n","    loss = 0.0\n","    batch_size = embedding1.size(0)\n","    for i in range(batch_size):\n","        emb1 = embedding1[i].unsqueeze(0)\n","        emb2 = embedding2[i].unsqueeze(0)\n","        label = labels[i]\n","        if label == 1:\n","            # For positive pairs, compute Euclidean distance between embeddings\n","            loss += F.pairwise_distance(emb1, emb2).pow(2)\n","        else:\n","            # For negative pairs, enforce margin by computing hinge loss\n","            loss += F.relu(margin - F.pairwise_distance(emb1, emb2)).pow(2)\n","    # Average the loss over the batch\n","    return loss / batch_size"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4-tl334D_6_v"},"outputs":[],"source":["class RelationExtractionGNN(torch.nn.Module):\n","    \"\"\"\n","    Relation Extraction Graph Neural Network model.\n","\n","    Args:\n","        num_node_features (int): Number of node features.\n","        num_classes (int): Number of relation classes.\n","\n","    Attributes:\n","        conv1 (GATConv): Graph attention layer 1.\n","        conv2 (GATConv): Graph attention layer 2.\n","        fc1 (torch.nn.Linear): Fully connected layer 1.\n","        fc_relation_pred (torch.nn.Linear): Fully connected layer for relation prediction.\n","    \"\"\"\n","    def __init__(self, num_node_features, num_classes):\n","        super(RelationExtractionGNN, self).__init__()\n","        # Define layers\n","        self.conv1 = GATConv(num_node_features, 128)\n","        self.conv2 = GATConv(128, 64)\n","        self.fc1 = torch.nn.Linear(64, 32)\n","        self.fc_relation_pred = torch.nn.Linear(32, num_classes)\n","\n","    def forward(self, x, edge_index, batch):\n","        \"\"\"\n","        Forward pass of the model.\n","\n","        Args:\n","            x (torch.Tensor): Node features.\n","            edge_index (torch.Tensor): Graph edge indices.\n","            batch (torch.Tensor): Batch indices.\n","\n","        Returns:\n","            tuple: Tuple containing embeddings and relation predictions.\n","        \"\"\"\n","        # Graph convolution layers with ReLU activation\n","        x = F.relu(self.conv1(x, edge_index))\n","        x = F.relu(self.conv2(x, edge_index))\n","        # Global pooling operation\n","        x = global_mean_pool(x, batch)\n","        # Fully connected layer with ReLU activation\n","        embedding = F.relu(self.fc1(x))\n","        # Final fully connected layer for relation prediction\n","        relation_pred = self.fc_relation_pred(embedding)\n","        return embedding, relation_pred\n","\n","# Instantiate the RelationExtractionGNN model\n","model = RelationExtractionGNN(num_node_features=58, num_classes=42)"]},{"cell_type":"markdown","metadata":{"id":"CWsrhvyn1uus"},"source":["### 7. Training and Saving the GNN Model\n","\n","Here we detail the training procedure for our model. We create a DataLoader for graph pairs, set up the loss function, and configure the optimizer for the training process. The model undergoes training over several epochs, during which it learns to optimize both the contrastive loss (to distinguish between similar and dissimilar graph pairs) and the relation classification loss. After training, we save the trained model."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hNAuZu610XKt","outputId":"cff7308e-af83-4965-f900-a207fa6276c3"},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/srikanthkuppili/anaconda3/lib/python3.11/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n","  warnings.warn(out)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 1, Loss: 6.295862104071945\n","Epoch: 2, Loss: 4.866859180028321\n"]}],"source":["# Create DataLoader for balanced pairs of graphs\n","pair_loader = DataLoader(balanced_pairs, batch_size=32, shuffle=True)\n","# Define cross-entropy loss criterion\n","criterion = CrossEntropyLoss()\n","# Define Adam optimizer for model parameters\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","# Margin for contrastive loss\n","margin = 2.0\n","\n","# Iterate over epochs\n","for epoch in range(1, 3):\n","    total_loss = 0\n","    total_loss_epoch = 0\n","    # Iterate over data batches\n","    for data in pair_loader:\n","        graph1, graph2, labels = data\n","        optimizer.zero_grad()\n","        # Forward pass for the first graph\n","        embedding1, output1 = model(graph1.x, graph1.edge_index, graph1.batch)\n","        # Forward pass for the second graph\n","        embedding2, output2 = model(graph2.x, graph2.edge_index, graph2.batch)\n","        # Compute contrastive loss between embeddings\n","        c_loss = contrastive_loss(embedding1, embedding2, labels, margin=2.0)\n","        # Extract relation labels from edge attributes\n","        relation_labels1 = graph1.edge_attr.max(dim=1)[1]\n","        relation_labels2 = graph2.edge_attr.max(dim=1)[1]\n","        # Compute relation classification loss\n","        loss_relation1 = criterion(output1, relation_labels1)\n","        loss_relation2 = criterion(output2, relation_labels2)\n","        # Total loss is a combination of contrastive and relation classification losses\n","        total_loss = loss_relation1 + loss_relation2 + c_loss\n","        # Backpropagation and optimization step\n","        total_loss.backward()\n","        optimizer.step()\n","        total_loss_epoch += total_loss.item()\n","    # Print epoch and average loss over the data loader\n","    print(f'Epoch: {epoch}, Loss: {total_loss_epoch / len(pair_loader)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hwJw7Wa7H9-f"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pdBVXDwbLjGI"},"outputs":[],"source":["# Define the path where the trained model will be saved\n","model_path = 'my_trained_model.pth'\n","\n","# Save the model's state dictionary to the specified path\n","torch.save({\n","    'model_state_dict': model.state_dict()  # Saving the state dictionary containing model parameters\n","}, model_path)"]},{"cell_type":"markdown","metadata":{"id":"c8dM3EAj2tYX"},"source":["### 8. Evaluating the GNN Model on Test Data\n","\n","Next, we evaluate the performance of our model on unseen test data. We load the pre-trained model, preprocess the test dataset, create graph data from it, and assemble a DataLoader for evaluation. The evaluation process calculates the model's accuracy, precision, recall, and F1 score over the test dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v_YeNTwsKVTA","outputId":"71d90c39-325f-4855-d305-b72fbbf26edc"},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["# Define the model architecture with the same parameters as used during training\n","loaded_model = RelationExtractionGNN(num_node_features=58, num_classes=42)\n","\n","# Load the saved model parameters (state_dict) from the specified file\n","checkpoint = torch.load(model_path)\n","\n","# Update the model's parameters with the loaded state_dict\n","loaded_model.load_state_dict(checkpoint['model_state_dict'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a5TwX5ti0XH2"},"outputs":[],"source":["# Preprocess the test dataset using the pre-defined tag indices for POS and NER tags\n","preprocessed_test_data = preprocess_dataset(test_data, pos_tag_to_index, ner_tag_to_index)\n","\n","# Create graphs from the preprocessed test data\n","graphs_test = create_graphs(preprocessed_test_data)\n","\n","# Create a DataLoader for the test graphs with a batch size of 32 and no shuffling\n","test_loader = DataLoader(graphs_test, batch_size=32, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pu-xuWhM0XFR"},"outputs":[],"source":["def evaluate_model(model, loader):\n","    \"\"\"\n","    Evaluate the performance of the model on the given data loader.\n","\n","    Args:\n","    - model: The model to be evaluated\n","    - loader: DataLoader containing the dataset for evaluation\n","\n","    Returns:\n","    - true_labels: List of true relation labels\n","    - predicted_labels: List of predicted relation labels\n","    - accuracy: Accuracy of the model on the evaluation dataset\n","    - precision: Precision score of the model on the evaluation dataset\n","    - recall: Recall score of the model on the evaluation dataset\n","    - f1: F1 score of the model on the evaluation dataset\n","    \"\"\"\n","    model.eval()  # Set the model to evaluation mode\n","    true_labels = []  # List to store true relation labels\n","    predicted_labels = []  # List to store predicted relation labels\n","\n","    with torch.no_grad():  # Turn off gradient tracking during evaluation\n","        for data in loader:  # Iterate through the data loader\n","            # Forward pass through the model to get embeddings and predictions\n","            embedding, output = model(data.x, data.edge_index, data.batch)\n","\n","            # Predicted labels are obtained by selecting the class with maximum probability\n","            preds = output.argmax(dim=1)\n","\n","            # True relation labels are decoded from one-hot encoding\n","            true_relations = data.edge_attr.argmax(dim=1)\n","\n","            # Extend the lists with true and predicted labels\n","            true_labels.extend(true_relations.tolist())\n","            predicted_labels.extend(preds.tolist())\n","\n","    # Calculate evaluation metrics\n","    accuracy = accuracy_score(true_labels, predicted_labels)\n","    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predicted_labels, average='weighted')\n","\n","    return true_labels, predicted_labels, accuracy, precision, recall, f1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tBwkHB96KCoI","outputId":"ae75ab28-7d77-4083-a448-8f47db001d5c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Accuracy: 0.7856\n","Precision: 0.6172\n","Recall: 0.7856\n","F1 Score: 0.6913\n"]},{"name":"stderr","output_type":"stream","text":["/Users/srikanthkuppili/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["# Evaluate the loaded model using the test data loader\n","true_labels, predicted_labels, accuracy, precision, recall, f1 = evaluate_model(loaded_model, test_loader)\n","\n","# Print the evaluation metrics: test accuracy, precision, recall, and F1 score\n","print(f\"Test Accuracy: {accuracy:.4f}\")\n","print(f\"Precision: {precision:.4f}\")\n","print(f\"Recall: {recall:.4f}\")\n","print(f\"F1 Score: {f1:.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"JY6dbtX325y3"},"source":["### 9. Scoring against Gold Standard TACRED Labels\n","\n","We save the predicted relation labels into a text file for comparison with gold standard annotations file. We use an external scoring script (`score.py`) provided with TACRED to evaluate the model's predictions against a set of ground truth labels (`test.gold`)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q4GtzluN-34u"},"outputs":[],"source":["index_to_relation = {value: key for key, value in relation_to_index.items()}\n","predicted_labels_forEval = [index_to_relation[label_id] for label_id in predicted_labels]\n","true_labels_forEval = [index_to_relation[label_id] for label_id in true_labels]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mITF5PFcSNA-","outputId":"57c1296d-851f-43ef-dfcf-70c704585af9"},"outputs":[{"data":{"text/plain":["{('no_relation', 'no_relation'): 12184,\n"," ('per:title', 'no_relation'): 500,\n"," ('org:top_members/employees', 'no_relation'): 346,\n"," ('org:country_of_headquarters', 'no_relation'): 108,\n"," ('per:parents', 'no_relation'): 88,\n"," ('per:age', 'no_relation'): 200,\n"," ('per:countries_of_residence', 'no_relation'): 148,\n"," ('per:children', 'no_relation'): 37,\n"," ('org:alternate_names', 'no_relation'): 213,\n"," ('per:charges', 'no_relation'): 103,\n"," ('per:cities_of_residence', 'no_relation'): 189,\n"," ('per:origin', 'no_relation'): 132,\n"," ('org:founded_by', 'no_relation'): 68,\n"," ('per:employee_of', 'no_relation'): 264,\n"," ('per:siblings', 'no_relation'): 55,\n"," ('per:alternate_names', 'no_relation'): 11,\n"," ('org:website', 'no_relation'): 26,\n"," ('per:religion', 'no_relation'): 47,\n"," ('per:stateorprovince_of_death', 'no_relation'): 14,\n"," ('org:parents', 'no_relation'): 62,\n"," ('org:subsidiaries', 'no_relation'): 44,\n"," ('per:other_family', 'no_relation'): 60,\n"," ('per:stateorprovinces_of_residence', 'no_relation'): 81,\n"," ('org:members', 'no_relation'): 31,\n"," ('per:cause_of_death', 'no_relation'): 52,\n"," ('org:member_of', 'no_relation'): 18,\n"," ('org:number_of_employees/members', 'no_relation'): 19,\n"," ('per:country_of_birth', 'no_relation'): 5,\n"," ('org:shareholders', 'no_relation'): 13,\n"," ('org:stateorprovince_of_headquarters', 'no_relation'): 51,\n"," ('per:city_of_death', 'no_relation'): 28,\n"," ('per:date_of_birth', 'no_relation'): 9,\n"," ('per:spouse', 'no_relation'): 66,\n"," ('org:city_of_headquarters', 'no_relation'): 82,\n"," ('per:date_of_death', 'no_relation'): 54,\n"," ('per:schools_attended', 'no_relation'): 30,\n"," ('org:political/religious_affiliation', 'no_relation'): 10,\n"," ('per:country_of_death', 'no_relation'): 9,\n"," ('org:founded', 'no_relation'): 37,\n"," ('per:stateorprovince_of_birth', 'no_relation'): 8,\n"," ('per:city_of_birth', 'no_relation'): 5,\n"," ('org:dissolved', 'no_relation'): 2}"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["# Count the occurrences of unique pairs of true and predicted labels\n","truePred_count = {}  # Dictionary to store the count of each unique pair\n","\n","# Iterate through true_labels and predicted_labels using zip\n","for comb in zip(true_labels_forEval,predicted_labels_forEval):\n","    truePred_count[comb] = truePred_count.get(comb, 0) + 1  # Increment the count for each unique pair or initialize to 1 if not present\n","\n","truePred_count  # Return the dictionary containing the count of each unique pair"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vhB0PHoJ_fzz"},"outputs":[],"source":["# Open a file in write mode to store predicted labels for evaluation\n","with open('predicted_labels_forEval.txt', 'w') as file:\n","    # Write each predicted label to the file, with each label on a separate line\n","    for label in predicted_labels_forEval:\n","        file.write(f\"{label}\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Zu_TlABAUdS","outputId":"0181f68f-da3d-4227-bca8-5a1407a5a731"},"outputs":[{"name":"stdout","output_type":"stream","text":["Per-relation statistics:\n","org:alternate_names                  P: 100.00%  R:   0.00%  F1:   0.00%  #: 213\n","org:city_of_headquarters             P: 100.00%  R:   0.00%  F1:   0.00%  #: 82\n","org:country_of_headquarters          P: 100.00%  R:   0.00%  F1:   0.00%  #: 108\n","org:dissolved                        P: 100.00%  R:   0.00%  F1:   0.00%  #: 2\n","org:founded                          P: 100.00%  R:   0.00%  F1:   0.00%  #: 37\n","org:founded_by                       P: 100.00%  R:   0.00%  F1:   0.00%  #: 68\n","org:member_of                        P: 100.00%  R:   0.00%  F1:   0.00%  #: 18\n","org:members                          P: 100.00%  R:   0.00%  F1:   0.00%  #: 31\n","org:number_of_employees/members      P: 100.00%  R:   0.00%  F1:   0.00%  #: 19\n","org:parents                          P: 100.00%  R:   0.00%  F1:   0.00%  #: 62\n","org:political/religious_affiliation  P: 100.00%  R:   0.00%  F1:   0.00%  #: 10\n","org:shareholders                     P: 100.00%  R:   0.00%  F1:   0.00%  #: 13\n","org:stateorprovince_of_headquarters  P: 100.00%  R:   0.00%  F1:   0.00%  #: 51\n","org:subsidiaries                     P: 100.00%  R:   0.00%  F1:   0.00%  #: 44\n","org:top_members/employees            P: 100.00%  R:   0.00%  F1:   0.00%  #: 346\n","org:website                          P: 100.00%  R:   0.00%  F1:   0.00%  #: 26\n","per:age                              P: 100.00%  R:   0.00%  F1:   0.00%  #: 200\n","per:alternate_names                  P: 100.00%  R:   0.00%  F1:   0.00%  #: 11\n","per:cause_of_death                   P: 100.00%  R:   0.00%  F1:   0.00%  #: 52\n","per:charges                          P: 100.00%  R:   0.00%  F1:   0.00%  #: 103\n","per:children                         P: 100.00%  R:   0.00%  F1:   0.00%  #: 37\n","per:cities_of_residence              P: 100.00%  R:   0.00%  F1:   0.00%  #: 189\n","per:city_of_birth                    P: 100.00%  R:   0.00%  F1:   0.00%  #: 5\n","per:city_of_death                    P: 100.00%  R:   0.00%  F1:   0.00%  #: 28\n","per:countries_of_residence           P: 100.00%  R:   0.00%  F1:   0.00%  #: 148\n","per:country_of_birth                 P: 100.00%  R:   0.00%  F1:   0.00%  #: 5\n","per:country_of_death                 P: 100.00%  R:   0.00%  F1:   0.00%  #: 9\n","per:date_of_birth                    P: 100.00%  R:   0.00%  F1:   0.00%  #: 9\n","per:date_of_death                    P: 100.00%  R:   0.00%  F1:   0.00%  #: 54\n","per:employee_of                      P: 100.00%  R:   0.00%  F1:   0.00%  #: 264\n","per:origin                           P: 100.00%  R:   0.00%  F1:   0.00%  #: 132\n","per:other_family                     P: 100.00%  R:   0.00%  F1:   0.00%  #: 60\n","per:parents                          P: 100.00%  R:   0.00%  F1:   0.00%  #: 88\n","per:religion                         P: 100.00%  R:   0.00%  F1:   0.00%  #: 47\n","per:schools_attended                 P: 100.00%  R:   0.00%  F1:   0.00%  #: 30\n","per:siblings                         P: 100.00%  R:   0.00%  F1:   0.00%  #: 55\n","per:spouse                           P: 100.00%  R:   0.00%  F1:   0.00%  #: 66\n","per:stateorprovince_of_birth         P: 100.00%  R:   0.00%  F1:   0.00%  #: 8\n","per:stateorprovince_of_death         P: 100.00%  R:   0.00%  F1:   0.00%  #: 14\n","per:stateorprovinces_of_residence    P: 100.00%  R:   0.00%  F1:   0.00%  #: 81\n","per:title                            P: 100.00%  R:   0.00%  F1:   0.00%  #: 500\n","\n","Final Score:\n","Precision (micro): 100.000%\n","   Recall (micro): 0.000%\n","       F1 (micro): 0.000%\n"]}],"source":["# Run the \"score.py\" script with the test gold labels file and the predicted labels file as arguments\n","!python score.py test.gold predicted_labels_forEval.txt"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}